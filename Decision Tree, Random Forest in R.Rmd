---
title: "Decision Tree and Random Forest"
author: "Krishna P Koirala"
date: "6/18/2018"
output:
    md_document:
     variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Why do we need Random Forests?

You might be familiar with the concept of Decision Trees -- a probabilistic predictive model which can be used to classify data in a wide array of applications. Decision Trees are created through observation of data points. A probabilistic model is created by observing the features present in each point labeled a certain class, and then associating probabilities to these features.

<center>
<a title="By Stephen Milborrow (Own work) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0) or GFDL (http://www.gnu.org/copyleft/fdl.html)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ACART_tree_titanic_survivors.png"><img width="320" alt="CART tree titanic survivors" src="https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png"/></a>
<font size="2">*Example of a Decision Tree for the Titanic dataset.*</font>
</center>

Decision Trees are very interesting because one can follow the structure created to understand how the class was inferred. However, this kind of model is not without its own problems. One of the main problems is what is called **overfitting**. Overfitting happens when the process of creating the tree makes it so that the tree is extremely ramified and complex -- this means that the model will not generalize correctly.

This can mean that the data points are too varied, or maybe that there are too many features to be analyzed at once. However, if we cut down the number of data points or features, this might make our model worse. So, we would need another kind of solution for this problem.

## What are Random Forests?

Random Forests are one of the proposed solutions. As one might infer from its name, Random Forests are composed of multiple Decision Trees. This makes them part of a family of models -- that are composed of other models working in tandem -- called **ensemble learning models**. The main concept behind Random Forests is that, if you partition the data that would be used to create a single decision tree into different parts, create one tree for each of these partitions, and then use a method to "average" the results of all of these different trees, you should end up with a better model. In the case of trees used for classification, this "average" is the **mode** of the set of trees in the forest. For regression, the "average" is the **mean** of the set of trees in the forest.

The main mechanism behind Random Forests is **bagging**, which is shorthand for **bootstrap aggregating**. Bagging is the concept of randomly sampling some data from a dataset, but **with replacement**. What this means in practice is that there is some amount of data that will be repeated in each partition, and some amount of data that will not be represented in the samples -- about 63% of the unique examples are kept -- this makes it so that the model generated for that bag is able to generalize better to some degree. Each partition of data of our training data for the Random Forest applies this concept.

<center>
<img src="https://ibm.box.com/shared/static/5m7lep2u6fzt6ors1b0kpgv0jtzh3z7z.png" width="480">
<font size="2">*Bagging example. Notice how some data points are repeated -- this is intentional!*</font>
</center>

You might be asking yourself what happens to the data that is not present in the "bags". This data, aptly called *Out-Of-Bag Data*, serves as a kind of **testing data for the generated model** -- which serves as validation that our model works!

Additionally, Random Forests are created using **feature bagging** as well, which makes it so that there are no problems of overfitting due to a large amount of features for a small amount of data. For example, if a few features are very strong predictors, they will be present in a large amount of "bags", and these bags will become correlated. However, this also makes it so that the Random Forest itself does not focus only on what strongly predicts the data that it was fed, making the model generalize better. Traditionally, a dataset with a number $f$ of features will have $\left\lceil{\sqrt[2]{f}}\ \right\rceil$ features in each partition.

<center>
<img src="https://ibm.box.com/shared/static/a4b0d3eg7vtuh8wipj9eo4bat9szow67.png" width="720">
<font size="2">*Example of a Random Forest. Don't forget that the bags can have repeated data points!*</font>
</center>

---

## Using Random Forests in R
Now that you know what Random Forests are, we can move on to use them in R. Conveniently enough, CRAN (R's library repository) has a library ready for us -- aptly named `randomForest`. However, we first need to install it. You can do that by running the code cell below.

```{r}
library(rpart)
```

```{r}
str(kyphosis)
```

```{r}
sample(kyphosis)
```

```{r}
# For model building the target variable is factor and other variables are integers
tree <- rpart(Kyphosis~., method = 'class', data = kyphosis)
# There are lots of functions to examin the results of this model
# eg: printcp(fit), plotcp(fit), rsq.rpart(fit),print(fit), summary(fit), plot(fit), text(fit), post(fit, file =)
```

# Examin the model

```{r}
printcp(tree)
```

```{r}
plot(tree, uniform = TRUE, main = 'Kyphosis Tree')
text(tree, use.n = T, all = T)
```

```{r}
#install.packages('rpart.plot')
library(rpart.plot)
```

rpart.plot package gives much nicer plot than above
```{r}
# function of rpart.plot package
prp(tree)
```

# Using random forest for making model

```{r}
library(randomForest)
```

```{r}
rf.model <- randomForest(Kyphosis~., data = kyphosis)
```

```{r}
rf.model
```

There are lots of components I can grab from the model by giving $ sign

```{r}
# eg
rf.model$confusion
```


